{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc76b1b",
   "metadata": {},
   "source": [
    "# [Hands-on] Modular and Pipeline Building for Data Management with Kedro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6d45a",
   "metadata": {},
   "source": [
    "* online document: [link](https://kedro.readthedocs.io/en/stable/index.html)\n",
    "* github repository: [link](https://github.com/kedro-org/kedro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4518d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swyang/anaconda3/lib/python3.9/site-packages/flask/json/__init__.py:31: DeprecationWarning: Importing 'itsdangerous.json' is deprecated and will be removed in ItsDangerous 2.1. Use Python's 'json' module instead.\n",
      "  _slash_escape = \"\\\\/\" not in _json.dumps(\"/\")\n",
      "/home/swyang/anaconda3/lib/python3.9/site-packages/flask/json/__init__.py:61: DeprecationWarning: Importing 'itsdangerous.json' is deprecated and will be removed in ItsDangerous 2.1. Use Python's 'json' module instead.\n",
      "  class JSONEncoder(_json.JSONEncoder):\n",
      "/home/swyang/anaconda3/lib/python3.9/site-packages/flask/json/__init__.py:103: DeprecationWarning: Importing 'itsdangerous.json' is deprecated and will be removed in ItsDangerous 2.1. Use Python's 'json' module instead.\n",
      "  class JSONDecoder(_json.JSONDecoder):\n"
     ]
    }
   ],
   "source": [
    "from kedro.io import DataCatalog, MemoryDataSet\n",
    "from kedro.pipeline import node, pipeline\n",
    "from kedro.runner import SequentialRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3245258",
   "metadata": {},
   "source": [
    "## 1. Node\n",
    "\n",
    "A `node` is a Kedro concept. It is a wrapper for a Python function that names the inputs and outputs of that function. It is the building block of a pipeline. Nodes can be linked when the output of one node is the input of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6766843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a node in the pipeline by providing a function to be called\n",
    "along with variable names for inputs and/or outputs.\n",
    "    \n",
    "Args:\n",
    "    func: A function that corresponds to the node logic. The function\n",
    "        should have at least one input or output.\n",
    "    inputs: The name or the list of the names of variables used as inputs\n",
    "        to the function. The number of names should match the number of\n",
    "        arguments in the definition of the provided function. When\n",
    "        Dict[str, str] is provided, variable names will be mapped to\n",
    "        function argument names.\n",
    "    outputs: The name or the list of the names of variables used as outputs\n",
    "        to the function. The number of names should match the number of\n",
    "        outputs returned by the provided function. When Dict[str, str]\n",
    "        is provided, variable names will be mapped to the named outputs the\n",
    "        function returns.\n",
    "    name: Optional node name to be used when displaying the node in logs or\n",
    "        any other visualisations.\n",
    "    tags: Optional set of tags to be applied to the node.\n",
    "    confirms: Optional name or the list of the names of the datasets\n",
    "        that should be confirmed. This will result in calling ``confirm()``\n",
    "        method of the corresponding data set instance. Specified dataset\n",
    "        names do not necessarily need to be present in the node ``inputs``\n",
    "        or ``outputs``.\n",
    "    namespace: Optional node namespace.\n",
    "\n",
    "Returns:\n",
    "    A Node object with mapped inputs, outputs and function.\n",
    "\n",
    "Example:\n",
    "::\n",
    "    >>> import pandas as pd\n",
    "    >>> import numpy as np\n",
    "    >>>\n",
    "    >>> def clean_data(cars: pd.DataFrame,\n",
    "    >>>                boats: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    >>>     return dict(cars_df=cars.dropna(), boats_df=boats.dropna())\n",
    "    >>>\n",
    "    >>> def halve_dataframe(data: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    >>>     return np.array_split(data, 2)\n",
    "    >>>\n",
    "    >>> nodes = [\n",
    "    >>>     node(clean_data,\n",
    "    >>>          inputs=['cars2017', 'boats2017'],\n",
    "    >>>          outputs=dict(cars_df='clean_cars2017',\n",
    "    >>>                       boats_df='clean_boats2017')),\n",
    "    >>>     node(halve_dataframe,\n",
    "    >>>          'clean_cars2017',\n",
    "    >>>          ['train_cars2017', 'test_cars2017']),\n",
    "    >>>     node(halve_dataframe,\n",
    "    >>>          dict(data='clean_boats2017'),\n",
    "    >>>          ['train_boats2017', 'test_boats2017'])\n",
    "    >>> ]\n",
    "\"\"\"\n",
    "\n",
    "# Prepare first node\n",
    "def greeting_func():\n",
    "    return \"Hello\"\n",
    "\n",
    "greeting_node = node(\n",
    "    func=greeting_func, inputs=None, outputs=\"greeting_words\"\n",
    ")\n",
    "\n",
    "# Prepare second node\n",
    "def introducing_func(greeting_words):\n",
    "    return f\"{greeting_words}, AIMedic!\"\n",
    "\n",
    "introducing_node = node(\n",
    "    func=introducing_func, inputs=\"greeting_words\", outputs=\"message\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1089a761",
   "metadata": {},
   "source": [
    "## 2. Pipeline\n",
    "A pipeline organises the dependencies and execution order of a collection of nodes, and connects inputs and outputs while keeping your code modular. The pipeline determines the node execution order by resolving dependencies and does not necessarily run the nodes in the order in which they are passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad59d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a ``Pipeline`` from a collection of nodes and/or ``Pipeline``s.\n",
    "Args:\n",
    "    pipe: The nodes the ``Pipeline`` will be made of. If you\n",
    "        provide pipelines among the list of nodes, those pipelines will\n",
    "        be expanded and all their nodes will become part of this\n",
    "        new pipeline.\n",
    "    inputs: A name or collection of input names to be exposed as connection points\n",
    "        to other pipelines upstream. This is optional; if not provided, the\n",
    "        pipeline inputs are automatically inferred from the pipeline structure.\n",
    "        When str or Set[str] is provided, the listed input names will stay\n",
    "        the same as they are named in the provided pipeline.\n",
    "        When Dict[str, str] is provided, current input names will be\n",
    "        mapped to new names.\n",
    "        Must only refer to the pipeline's free inputs.\n",
    "    outputs: A name or collection of names to be exposed as connection points\n",
    "        to other pipelines downstream. This is optional; if not provided, the\n",
    "        pipeline inputs are automatically inferred from the pipeline structure.\n",
    "        When str or Set[str] is provided, the listed output names will stay\n",
    "        the same as they are named in the provided pipeline.\n",
    "        When Dict[str, str] is provided, current output names will be\n",
    "        mapped to new names.\n",
    "        Can refer to both the pipeline's free outputs, as well as\n",
    "        intermediate results that need to be exposed.\n",
    "    parameters: A name or collection of parameters to namespace.\n",
    "        When str or Set[str] are provided, the listed parameter names will stay\n",
    "        the same as they are named in the provided pipeline.\n",
    "        When Dict[str, str] is provided, current parameter names will be\n",
    "        mapped to new names.\n",
    "        The parameters can be specified without the `params:` prefix.\n",
    "    tags: Optional set of tags to be applied to all the pipeline nodes.\n",
    "    namespace: A prefix to give to all dataset names,\n",
    "        except those explicitly named with the `inputs`/`outputs`\n",
    "        arguments, and parameter references (`params:` and `parameters`).\n",
    "\n",
    "Raises:\n",
    "    ModularPipelineError: When inputs, outputs or parameters are incorrectly\n",
    "        specified, or they do not exist on the original pipeline.\n",
    "    ValueError: When underlying pipeline nodes inputs/outputs are not\n",
    "        any of the expected types (str, dict, list, or None).\n",
    "\n",
    "Returns:\n",
    "    A new ``Pipeline`` object.\n",
    "\"\"\"\n",
    "\n",
    "# Assemble nodes to a pipeline\n",
    "my_pipeline = pipeline(\n",
    "    pipe=[greeting_node, introducing_node]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a793f",
   "metadata": {},
   "source": [
    "## 3. DataCatalog\n",
    "A `DataCatalog` is a Kedro concept. It is the registry of all data sources that the project can use. It maps the names of node inputs and outputs as keys in a `DataSet`, which is a Kedro class that can be specialised for different types of data storage. Kedro uses a `MemoryDataSet` for data that is simply stored in-memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b89bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"``DataCatalog`` stores instances of ``AbstractDataSet``\n",
    "implementations to provide ``load`` and ``save`` capabilities from\n",
    "anywhere in the program. To use a ``DataCatalog``, you need to\n",
    "instantiate it with a dictionary of data sets. Then it will act as a\n",
    "single point of reference for your calls, relaying load and save\n",
    "functions to the underlying data sets.\n",
    "\n",
    "Args:\n",
    "    data_sets: A dictionary of data set names and data set instances.\n",
    "    feed_dict: A feed dict with data to be added in memory.\n",
    "    layers: A dictionary of data set layers. It maps a layer name\n",
    "        to a set of data set names, according to the\n",
    "        data engineering convention. For more details, see\n",
    "        https://kedro.readthedocs.io/en/stable/faq/faq.html#what-is-data-engineering-convention\n",
    "        \n",
    "Example:\n",
    "::\n",
    "    >>> from kedro.extras.datasets.pandas import CSVDataSet\n",
    "    >>>\n",
    "    >>> cars = CSVDataSet(filepath=\"cars.csv\",\n",
    "    >>>                   load_args=None,\n",
    "    >>>                   save_args={\"index\": False})\n",
    "    >>> io = DataCatalog(data_sets={'cars': cars})\n",
    "\"\"\"\n",
    "\n",
    "# Prepare a data catalog\n",
    "INPUT_DATA_HASH = {\n",
    "    \"greeting_words\": MemoryDataSet()\n",
    "}\n",
    "\n",
    "data_catalog = DataCatalog(\n",
    "    data_sets=INPUT_DATA_HASH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7e2f7",
   "metadata": {},
   "source": [
    "## 4. Runner\n",
    "The `Runner` is an object that runs the pipeline. Kedro resolves the order in which the nodes are executed:\n",
    "\n",
    "1. Kedro first executes `greeting_node`. This runs `greeting_func`, which takes no input but outputs the string “Hello”.\n",
    "\n",
    "2. The output string is stored in the `MemoryDataSet` named `greeting_words`.\n",
    "\n",
    "3. Kedro then executes the second node, `introducing_node`. This loads the `greeting_words` dataset and injects it into the `introducing_func` function.\n",
    "\n",
    "4. The function joins the input salutation with “AIMedic!” to form the output string “Hello AIMedic!”\n",
    "\n",
    "5. The output of the pipeline is returned in a dictionary with key `message`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2d8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The method implementing sequential pipeline running.\n",
    "        \n",
    "Args:\n",
    "    pipeline: The ``Pipeline`` to run.\n",
    "    catalog: The ``DataCatalog`` from which to fetch data.\n",
    "    hook_manager: The ``PluginManager`` to activate hooks.\n",
    "    session_id: The id of the session.\n",
    "\n",
    "Raises:\n",
    "    Exception: in case of any downstream node failure.\n",
    "\"\"\"\n",
    "\n",
    "# Create a runner to run the pipeline\n",
    "runner = SequentialRunner()\n",
    "\n",
    "# Rune the pipeline\n",
    "output_data_dict = runner.run(my_pipeline, data_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956bc8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Hello, AIMedic!'}\n"
     ]
    }
   ],
   "source": [
    "# Print outputs of the pipeline\n",
    "print(output_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25566f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
